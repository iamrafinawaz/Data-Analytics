{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkxFpTk2qg2J5Owe6EUE9X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamrafinawaz/Data-Analytics/blob/main/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Simple Linear Regression models the relationship between a dependent variable and a single independent variable using a linear equation.\n",
        "\n",
        "2. Key assumptions of Simple Linear Regression: Linearity, Independence, Homoscedasticity, and Normality of residuals.\n",
        "\n",
        "3. The coefficient \\( m \\) represents the slope, indicating the change in \\( Y \\) for a one-unit change in \\( X \\).\n",
        "\n",
        "4. The intercept \\( c \\) is the value of \\( Y \\) when \\( X \\) is zero.\n",
        "\n",
        "5. The slope \\( m \\) is calculated to minimize the sum of squared differences between observed and predicted values.\n",
        "\n",
        "6. The least squares method minimizes the sum of the squares of the residuals to find the best-fitting line.\n",
        "\n",
        "7. \\( R^2 \\) indicates the proportion of variance in \\( Y \\) explained by \\( X \\), ranging from 0 to 1.\n",
        "\n",
        "8. Multiple Linear Regression models the relationship between a dependent variable and multiple independent variables.\n",
        "\n",
        "9. Simple Linear Regression involves one independent variable, while Multiple Linear Regression involves two or more.\n",
        "\n",
        "10. Key assumptions of Multiple Linear Regression: Linearity, Independence, Homoscedasticity, Normality of residuals, and No multicollinearity.\n",
        "\n",
        "11. Heteroscedasticity occurs when residual variance is not constant, leading to inefficient estimates and invalid hypothesis tests.\n",
        "\n",
        "12. Improve a model with high multicollinearity by removing correlated predictors, combining predictors, using regularization techniques, or applying PCA.\n",
        "\n",
        "13. Common techniques for transforming categorical variables: One-hot encoding, Label encoding, and Binary encoding.\n",
        "\n",
        "14. Interaction terms capture the combined effect of two or more predictors on the dependent variable.\n",
        "\n",
        "15. In Multiple Linear Regression, the intercept represents the expected value of \\( Y \\) when all predictors are zero.\n",
        "\n",
        "16. The slope indicates the rate of change in the dependent variable for a one-unit change in the predictor.\n",
        "\n",
        "17. The intercept provides the baseline value of the dependent variable when all predictors are zero.\n",
        "\n",
        "18. \\( R^2 \\) does not account for model complexity or overfitting and does not indicate if predictors are significant.\n",
        "\n",
        "19. A large standard error indicates less precision in the estimate of the coefficient, suggesting variability in the data.\n",
        "\n",
        "20. Heteroscedasticity is identified by a funnel shape in residual plots. Addressing it ensures valid hypothesis tests and efficient estimates.\n",
        "\n",
        "21. A high \\( R^2 \\) but low adjusted \\( R^2 \\) suggests that the model may include irrelevant predictors, leading to overfitting.\n",
        "\n",
        "22. Scaling ensures that all predictors contribute equally to the model, improving numerical stability and interpretability.\n",
        "\n",
        "23. Polynomial regression models the relationship between the dependent variable and the independent variable as an nth-degree polynomial.\n",
        "\n",
        "24. Polynomial regression fits a nonlinear relationship, while linear regression fits a straight line.\n",
        "\n",
        "25. Polynomial regression is used when the relationship between variables is nonlinear and cannot be captured by a straight line.\n",
        "\n",
        "26. The general equation includes terms up to the nth power of the independent variable.\n",
        "\n",
        "27. Yes, polynomial regression can be extended to include multiple predictors.\n",
        "\n",
        "28. Polynomial regression can overfit the data, especially with high-degree polynomials, and may be sensitive to outliers.\n",
        "\n",
        "29. Methods to evaluate model fit: Cross-validation, Adjusted \\( R^2 \\), and AIC/BIC criteria.\n",
        "\n",
        "30. Visualization helps to understand the fit of the model and identify potential overfitting or underfitting.\n",
        "\n",
        "31. Polynomial regression is implemented using libraries like NumPy and scikit-learn, which provide functions for fitting polynomial models.\n",
        "\n"
      ],
      "metadata": {
        "id": "U2FakUKaIl-1"
      }
    }
  ]
}